%! TeX program = xelatex
\documentclass[zihao=-4, fontset=windows]{ctexart}
\input{./note-setup-leftsidebox.tex}

\title{生成式AI学习笔记}
\author{张哲源}
\date{\today}

\begin{document}

\maketitle

\section{变分自编码器 (Variational Autoencoder, VAE)}

\subsection*{符号说明 (Notation)}
\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{符号} & \textbf{含义} \\
\midrule
$\mathcal{D}$ & 数据集，$\mathcal{D} = \{ \mathbf{x}^{(1)}, \dots, \mathbf{x}^{(n)} \}$ \\
$\mathbf{x}^{(i)}$ & 第 $i$ 个数据样本，维度为 $d$ \\
$\mathbf{z}$ & 瓶颈层学习到的压缩隐变量 (Latent Code) \\
$g_\phi(\cdot)$ & 编码器函数，参数为 $\phi$ \\
$f_\theta(\cdot)$ & 解码器函数，参数为 $\theta$ \\
$q_\phi(\mathbf{z}|\mathbf{x})$ & 推断网络（近似后验），概率编码器 \\
$p_\theta(\mathbf{x}|\mathbf{z})$ & 生成网络（似然），概率解码器 \\
\bottomrule
\end{tabular}
\end{center}

\subsection{前置知识：自编码器及其变体 (Autoencoders)}

\begin{definition}[自动编码器]
	自动编码器（Autoencoder, AE）是一种无监督学习模型，其核心目标是学习一个恒等函数（Identity Function），即输出 $\mathbf{x}' \approx \mathbf{x}$。它通过一个具有窄瓶颈层（Bottleneck）的神经网络将高维输入压缩为低维编码（降维），再从该编码重构出原始输入。
\end{definition}

\subsubsection{基本自动编码器 (Autoencoder)}
自动编码器通常包含两个部分：
\begin{itemize}
	\item \textbf{编码器 (Encoder)} $g_\phi(\cdot)$：将高维输入 $\mathbf{x}$ 映射为低维隐变量 $\mathbf{z} = g_\phi(\mathbf{x})$。
	\item \textbf{解码器 (Decoder)} $f_\theta(\cdot)$：将隐变量 $\mathbf{z}$ 映射回重构数据 $\mathbf{x}' = f_\theta(\mathbf{z})$。
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/autoencoder-architecture.png}
    \caption{自动编码器模型架构示意图}
\end{figure}

训练目标是最小化重构误差，例如均方误差 (MSE)：
\begin{equation}
	L_{\text{AE}}(\theta, \phi) = \frac{1}{n}\sum_{i=1}^n (\mathbf{x}^{(i)} - f_\theta(g_\phi(\mathbf{x}^{(i)})))^2
\end{equation}

\subsubsection{去噪自动编码器 (Denoising Autoencoder, DAE)}
为了避免过拟合（即单纯地记忆输入数据），去噪自动编码器引入了“破坏”机制。
\begin{note}[核心思想]
	DAE 在输入 $\mathbf{x}$ 中主动加入噪声得到 $\tilde{\mathbf{x}}$（例如将部分像素置零，类似 Dropout），然后训练模型从损坏的 $\tilde{\mathbf{x}}$ 中恢复出\textbf{原始的}无噪数据 $\mathbf{x}$。这迫使模型学习数据维度间的依赖关系，从而提取更鲁棒的特征。
\end{note}
损失函数调整为：
\begin{equation}
	L_{\text{DAE}}(\theta, \phi) = \frac{1}{n} \sum_{i=1}^n (\mathbf{x}^{(i)} - f_\theta(g_\phi(\tilde{\mathbf{x}}^{(i)})))^2
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/denoising-autoencoder-architecture.png}
    \caption{去噪自动编码器模型架构示意图}
\end{figure}

\subsubsection{稀疏自动编码器 (Sparse Autoencoder, SAE)}
稀疏自动编码器通过在损失函数中增加稀疏约束，强制隐层神经元在大部分时间处于“未激活”状态。
假设隐藏层神经元 $j$ 的平均激活度为 $\hat{\rho}_j$，我们希望它接近一个很小的稀疏参数 $\rho$（如 0.05）。通常使用 KL 散度作为惩罚项：
\begin{equation}
	L_{\text{SAE}}(\theta) = L(\theta) + \beta \sum_{j=1}^{s_l} D_{\text{KL}}(\rho \| \hat{\rho}_j)
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/kl-metric-sparse-autoencoder.png}
    \caption{伯努利分布均值为 $\rho=0.25$ 和均值为 $0 \leq \hat{\rho} \leq 1$ 的伯努利分布之间的 KL 散度}
\end{figure}

此外，还有 \textbf{$k$-稀疏自动编码器}（$k$-Sparse Autoencoder）。其前向传播步骤如下：
\begin{enumerate}
    \item 运行编码器得到潜在代码 $\mathbf{z} = g(\mathbf{x})$。
    \item 对 $\mathbf{z}$ 进行排序，仅保留前 $k$ 个最大的激活值，将其余值置零，得到 $\mathbf{z}' = \text{Sparsify}(\mathbf{z})$。
    \item 计算重构损失 $L = \|\mathbf{x} - f(\mathbf{z}')\|_2^2$。
\end{enumerate}
注意：反向传播时，梯度仅通过这前 $k$ 个激活的神经元传递。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/k-sparse-autoencoder.png}
    \caption{对不同稀疏级别 k 的 MNIST 数据集学习的 k-稀疏自动编码器的滤波器}
\end{figure}

\subsubsection{收缩自动编码器 (Contracting Autoencoder, CAE)}
收缩自动编码器旨在使学习到的表示对输入的微小扰动具有鲁棒性。它在损失函数中加入雅可比矩阵（Jacobian Matrix）的 Frobenius 范数作为惩罚项，抑制编码器激活值对输入的敏感度：
\begin{equation}
	\|J_f(\mathbf{x})\|_F^2 = \sum_{ij} \Big( \frac{\partial h_j(\mathbf{x})}{\partial x_i} \Big)^2
\end{equation}
这促使模型学习到的特征分布在低维流形上，而对流形正交方向的变化保持不变。
\subsection{VAE 原理详解 (The Principle of VAE)}

\begin{definition}
	变分自编码器（Variational Autoencoder, VAE）是一种强大的深度生成模型，它巧妙地将深度学习的表示能力与贝叶斯推断的概率框架相结合。与传统的自编码器（Autoencoder）不同，VAE并非学习一个从输入到编码的确定性映射，而是学习一个从输入到隐变量后验概率分布的映射。通过从这个学习到的分布中采样，VAE能够生成与训练数据相似但全新的数据样本。
\end{definition}

\noindent\textbf{关键词：}生成模型，变分自编码器，贝叶斯推断，证据下界，重参数化技巧

\subsubsection{引言 (Introduction)}

生成模型的根本目标是学习观测数据 $X$ 的真实概率分布 $P_{data}(X)$。一旦这个分布被成功建模，我们便可以从中采样，生成新的数据。
\marginpar{挑战}
早期的生成模型如受限玻尔兹曼机（RBM）面临训练困难的问题，而传统的自编码器（AE）虽能学习数据的有效压缩表示，但其隐空间（latent space）缺乏良好的结构性，无法直接用于生成新样本。

\begin{note}[VAE的核心思想]
	为了解决这一问题，Kingma和Welling在2013年提出了变分自编码器。VAE的核心思想是，不再将输入数据 $X$ 编码为一个确定的隐向量 $z$，而是将其编码为一个概率分布（通常是高斯分布）。具体来说，模型学习这个分布的参数（例如均值 $\mu$ 和方差 $\sigma^2$）。生成过程则变为：
	\begin{enumerate}
		\item 首先从这个分布中采样一个隐向量 $z$。
		\item 然后解码器（Decoder）基于这个 $z$ 来重构出数据 $\hat{X}$。
	\end{enumerate}
	通过这种方式，VAE不仅学习了如何重构数据，更重要的是，它学习到了一个结构化、连续的隐空间，使得我们可以在这个空间中进行采样和插值，从而生成多样化的新数据。
\end{note}

\subsubsection{模型架构与概率框架 (Model Architecture \& Probabilistic Framework)}
从概率的视角来看，VAE假设所有的数据样本 $X$ 是由一个我们无法直接观测到的隐变量 $z$ 生成的。这个生成过程包含以下步骤：
\begin{enumerate}
	\item 从一个简单的先验分布 $p(z)$ 中采样一个隐变量 $z$。通常，我们选择标准正态分布，即 $z \sim \mathcal{N}(0, I)$。
	\item 根据隐变量 $z$，通过一个以神经网络（解码器）参数化的条件概率分布 $p_\theta(X|z)$ 来生成数据 $X$。
\end{enumerate}

我们的最终目标是最大化观测数据的边际似然 $p(X)$，即所有可能隐变量 $z$ 生成 $X$ 的概率的积分：
\begin{equation}
	p(X) = \int p_\theta(X|z)p(z)dz
\end{equation}
然而，这个积分通常是难以计算的（intractable），因为它需要在整个高维的隐空间上进行。这就引出了变分推断（Variational Inference）的必要性。

\paragraph{VAE模型架构} VAE由两个核心部分组成，通常由深度神经网络实现：
\begin{itemize}
	\item \textbf{编码器 (Encoder)，或称推断网络 (Inference Network):}
	      \begin{itemize}
		      \item \textbf{输入：}数据样本 $X$。
		      \item \textbf{输出：}隐变量 $z$ 的后验分布 $p(z|X)$ 的近似分布 $q_\phi(z|X)$ 的参数。这里，$\phi$ 是编码器网络的参数。具体地，编码器输出近似后验分布的参数，例如高斯分布的均值 $\mu_X$ 和对数方差 $\log(\sigma_X^2)$。即 $q_\phi(z|X) = \mathcal{N}(z; \mu_X, \sigma_X^2 I)$。
	      \end{itemize}
	\item \textbf{解码器 (Decoder)，或称生成网络 (Generative Network):}
	      \begin{itemize}
		      \item \textbf{输入：}从分布 $q_\phi(z|X)$ 中采样的隐向量 $z$。
		      \item \textbf{输出：}重构的数据 $\hat{X}$，或者更准确地说，是条件概率分布 $p_\theta(X|z)$ 的参数。
	      \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/VAE-graphical-model.png}
    \caption{变分自动编码器中涉及的概率图模型}
\end{figure}

\subsubsection{核心推导：证据下界 (Derivation of the Evidence Lower Bound, ELBO)}
由于直接最大化 $p(X)$ 不可行，我们转而最大化它的一个下界，即\textbf{证据下界（ELBO）}。我们从对数边际似然 $\log p(X)$ 开始，引入近似后验 $q_\phi(z|X)$，并根据詹森不等式（Jensen's Inequality），得到：
\begin{equation}
	\begin{aligned}
		\log p(X) &\ge \int q_\phi(z|X) \log \frac{p_\theta(X|z)p(z)}{q_\phi(z|X)} dz                               \\
		          &= \mathbb{E}_{z \sim q_\phi(z|X)} \left[ \log \frac{p_\theta(X|z)p(z)}{q_\phi(z|X)} \right]
	\end{aligned}
\end{equation}
不等式的右侧就是证据下界 (ELBO)，记为 $\mathcal{L}(\phi, \theta; X)$。

\begin{note}[ELBO的分解]
	\begin{equation}
		\begin{aligned}
			\mathcal{L}(\phi, \theta; X) &= \mathbb{E}_{z \sim q_\phi(z|X)} \left[ \log p_\theta(X|z) + \log p(z) - \log q_\phi(z|X) \right] \\
			&= \underbrace{\mathbb{E}_{z \sim q_\phi(z|X)} [\log p_\theta(X|z)]}_{\text{重构项}} - \underbrace{D_{KL}(q_\phi(z|X) || p(z))}_{\text{KL散度正则化项}}
		\end{aligned}
	\end{equation}
\end{note}

\begin{note}[为什么使用反向 KL 散度？]
	我们通过最小化 $D_{KL}(q_\phi(z|X) || p_\theta(z|X))$ 来逼近真实后验，这被称为\textbf{反向 KL 散度}（Reverse KL）。
	\begin{itemize}
		\item \textbf{前向 KL ($D_{KL}(P||Q)$):} 要求近似分布 $Q$ 覆盖真实分布 $P$ 的所有非零区域。
		\item \textbf{反向 KL ($D_{KL}(Q||P)$):} 倾向于将近似分布 $Q$ “挤压”在真实分布 $P$ 的峰值下方。这使得 $q_\phi$ 能够捕捉到 $p_\theta$ 的主要模式（Mode-seeking behavior），对于生成任务更为稳健。
	\end{itemize}
\end{note}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/forward_vs_reversed_KL.png}
    \caption{前向和反向 KL 散度对匹配两个分布有不同的要求}
\end{figure}

\begin{note}[目标函数解读]
	VAE的训练目标是最大化ELBO，它由两部分构成：
	\begin{itemize}
		\item \textbf{重构项 (Reconstruction Term):} 该项最大化在给定隐变量 $z$ 的情况下，重构出原始数据 $X$ 的对数似然。这驱使解码器学习如何精确地从 $z$ 恢复 $X$。在实践中，常使用均方误差（MSE）或二元交叉熵（BCE）损失的负值。
		\item \textbf{KL散度正则化项 (KL Divergence Regularization Term):} 该项衡量近似后验分布 $q_\phi(z|X)$ 与先验分布 $p(z)$（通常是 $\mathcal{N}(0, I)$）之间的差异。最小化KL散度，相当于对编码器施加约束，使其产生的隐变量分布接近标准正态分布，从而使隐空间变得平滑和连续。
	\end{itemize}
	因此，训练目标可以概括为：\textbf{最大化重构质量}的同时，\textbf{保持隐空间分布的规整性}。
\end{note}

\subsubsection{关键技术：重参数化技巧 (The Reparameterization Trick)}
\begin{definition}[重参数化技巧]
	在目标函数中，从 $q_\phi(z|X)$ 中采样 $z$ 的过程是随机的，不可微分，这会阻碍梯度的反向传播。
	\textbf{重参数化技巧}通过将随机性与模型参数分离来解决此问题。对于高斯分布 $z \sim \mathcal{N}(z; \mu_X, \sigma_X^2 I)$，其采样过程可重写为：
	\begin{enumerate}
		\item 从一个固定的标准正态分布中采样噪声 $\epsilon \sim \mathcal{N}(0, I)$。
		\item 通过确定性变换生成 $z$：$z = \mu_X + \sigma_X \odot \epsilon$。
	\end{enumerate}
	这样，随机采样被移出计算图，梯度可以顺利地从损失函数回传到编码器的参数 $\phi$（即 $\mu_X$ 和 $\sigma_X$）。
\end{definition}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{images/reparameterization-trick.png}
	\caption{重参数化技巧示意图}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{images/vae-gaussian.png}
	\caption{假设多元高斯分布的变分自动编码器模型示意图}
\end{figure}



\subsubsection{结论与讨论 (Conclusion and Discussion)}
变分自编码器（VAE）通过最大化证据下界（ELBO）成功地将深度神经网络与概率图模型结合起来。其损失函数由重构损失和KL散度正则化项构成，分别保证了模型的重构能力和隐空间的良好结构。关键的重参数化技巧使得模型能够通过标准的梯度下降算法进行端到端的训练。

\begin{note}[VAE vs. GAN]
	与GAN相比，VAE的训练过程更稳定，能够显式地学习数据的概率密度。然而，VAE生成的样本通常比GAN生成的样本\textbf{模糊}，这是因为其损失函数倾向于覆盖数据的所有模式，导致在细节上有所妥协。
\end{note}

尽管如此，VAE作为一种 foundational 的深度生成模型，其思想，特别是对隐空间的概率建模，对后续的生成模型（包括扩散模型）产生了深远的影响。理解VAE是掌握现代生成式AI版图不可或缺的一步。




\subsection{进阶变分模型 (Advanced VAE Models)}

\subsubsection{Beta-VAE ($\beta$-VAE)}
\begin{definition}[解耦表示 (Disentangled Representation)]
	如果隐变量 $\mathbf{z}$ 中的每个维度仅对数据的一个生成因子敏感（例如一个维度控制“发色”，另一个控制“肤色”），且对其他因子保持不变，则称这种表示为解耦的。解耦表示具有极佳的可解释性。
\end{definition}

$\beta$-VAE 旨在通过调整 ELBO 中的 KL 散度项权重来发现解耦的潜在因子。无论是从直觉上增加 KL 惩罚，还是从约束优化的角度来看，都有坚实的理论基础。
我们希望最大化生成概率，同时约束后验分布与先验分布的距离小于 $\delta$：
\begin{equation}
	\max_{\phi, \theta} \mathbb{E}_{\mathbf{x}\sim\mathcal{D}}[\mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} \log p_\theta(\mathbf{x}|\mathbf{z})] \quad \text{subject to } D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x})\|p_\theta(\mathbf{z})) < \delta
\end{equation}
使用拉格朗日乘数法，这等价于最大化：
\begin{equation}
	\mathcal{F}(\theta, \phi, \beta) = \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} \log p_\theta(\mathbf{x}|\mathbf{z}) - \beta (D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x})\|p_\theta(\mathbf{z})) - \delta)
\end{equation}
去掉常数项，得到 $\beta$-VAE 的损失函数：
\begin{equation}
	L_{\beta\text{-VAE}} = - \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] + \beta D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p_\theta(\mathbf{z}))
\end{equation}
\begin{note}[$\beta$ 的作用]
	\begin{itemize}
		\item 当 $\beta=1$ 时，模型退化为标准的 VAE。
		\item 当 $\beta > 1$ 时，模型对隐变量施加了更强的独立性约束，鼓励学习更有效的、解耦的潜在编码。但这通常会带来重构质量与解耦程度之间的权衡（Trade-off）。
	\end{itemize}
\end{note}

\subsubsection{VQ-VAE (Vector Quantized VAE)}
传统的 VAE 使用连续的隐变量分布（如高斯分布），这可能不适合某些模态（如语言、推理）。VQ-VAE 引入了\textbf{离散潜在变量}。

\begin{note}[向量量化 (Vector Quantization)]
	VQ-VAE 维护一个“代码簿”（Codebook），包含 $K$ 个嵌入向量 $\mathbf{e}_i$。编码器的输出 $\mathbf{z}_e(\mathbf{x})$ 不再直接传给解码器，而是通过最近邻搜索映射到代码簿中最接近的嵌入向量 $\mathbf{e}_k$：
	\begin{equation}
		\mathbf{z}_q(\mathbf{x}) = \mathbf{e}_k, \quad \text{where } k = \arg\min_i \|\mathbf{z}_e(\mathbf{x}) - \mathbf{e}_i \|_2
	\end{equation}
\end{note}
由于 $\arg\min$ 操作不可导，VQ-VAE 使用直通估计器（Straight-Through Estimator）将解码器的梯度直接复制回编码器（即 $\nabla_z L \approx \nabla_{z_q} L$）。其总损失函数如下（其中 $\text{sg}[\cdot]$ 表示这个梯度的截断算子 stop gradient）：
\begin{equation}
	L = \underbrace{\|\mathbf{x} - D(\mathbf{e}_k)\|_2^2}_{\text{Reconstruction Loss}} + \underbrace{\|\text{sg}[E(\mathbf{x})] - \mathbf{e}_k\|_2^2}_{\text{VQ Loss}} + \underbrace{\beta \|E(\mathbf{x}) - \text{sg}[\mathbf{e}_k]\|_2^2}_{\text{Commitment Loss}}
\end{equation}
\textbf{代码簿更新 (EMA):} 为了更稳定地更新嵌入向量，可以使用指数移动平均 (Exponential Moving Average)：
\begin{equation}
	N_i^{(t)} = \gamma N_i^{(t-1)} + (1-\gamma)n_i^{(t)}, \quad \mathbf{m}_i^{(t)} = \gamma \mathbf{m}_i^{(t-1)} + (1-\gamma)\sum_{j} \mathbf{z}_{i,j}^{(t)}, \quad \mathbf{e}_i^{(t)} = \frac{\mathbf{m}_i^{(t)}}{N_i^{(t)}}
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{images/VQ-VAE.png}
	\caption{VQ-VAE 的架构}
\end{figure}

\textbf{VQ-VAE-2} 进一步引入了层级结构（Hierarchical VQ-VAE），分离局部纹理和全局形状信息，并结合自回归模型（如 PixelCNN）来学习离散编码的先验分布，生成了极高质量的图像。

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{images/VQ-VAE-2.png}
	\caption{层次 VQ-VAE 和多阶段图像生成的架构}
\end{figure}

\subsubsection{TD-VAE (Temporal Difference VAE)}
TD-VAE 专为序列数据设计，旨在解决传统序列模型推断效率低的问题。它基于三个核心思想：
\begin{enumerate}
	\item \textbf{状态空间模型 (State Space Model):} 假设观测序列由一系列不可见的隐状态控制。
	\item \textbf{信念状态 (Belief State):} 智能体应学会将过去的所有信息编码为一个信念状态 $b_t$，用于推理未来。
	\item \textbf{跳跃预测 (Jumpy Prediction):} 模型应具备根据当前信息直接预测遥远未来的能力，而无需一步步模拟中间过程。
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{images/TD-VAE-state-space.png}
	\caption{作为马尔可夫链模型的状态空间模型}
\end{figure}

TD-VAE 需要学习四种分布：
\begin{itemize}
	\item \textbf{解码器分布} $p_D(x_t|z_t)$: 从隐状态生成观测。
	\item \textbf{转移分布} $p_T(z_t|z_{t-1})$: 捕捉隐变量的序列依赖。
	\item \textbf{信念分布} $p_B(z_t|b_t)$: 从信念状态推断隐变量。
	\item \textbf{平滑分布} $p_S(z_{t-1}|z_t, b_{t-1}, b_t)$: 回溯推断。
\end{itemize}
TD-VAE 的最终目标函数是在两个时间戳 $t_1 < t_2$ 之间最大化以下期望（Jump Prediction）：
\begin{equation}
	J_{t_1, t_2} = \mathbb{E} [ \log p_D(x_{t_2}|z_{t_2}) + \log p_B(z_{t_1}|b_{t_1}) + \log p_T(z_{t_2}|z_{t_1}) - \log p_B(z_{t_2}|b_{t_2}) - \log p_S(z_{t_1}|z_{t_2}, b_{t_1}, b_{t_2}) ]
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{images/TD-VAE.png}
	\caption{TD-VAE 架构详细概览}
\end{figure}

\section{扩散模型 (Diffusion Models)}

\subsection*{符号说明 (Notation)}
\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{符号} & \textbf{含义} \\
\midrule
$\mathbf{x}_0$ & 真实数据样本 \\
$\mathbf{x}_t$ & 时间步 $t$ 的噪声样本 \\
$T$ & 总扩散步数 \\
$\beta_t$ & 方差调度表，控制加噪量 \\
$\alpha_t$ & $1 - \beta_t$ \\
$\bar{\alpha}_t$ & $\prod_{i=1}^t \alpha_i$, 累积乘积 \\
$\boldsymbol{\epsilon}$ & 标准高斯噪声, $\mathcal{N}(\mathbf{0}, \mathbf{I})$ \\
$\boldsymbol{\epsilon}_\theta$ & 噪声预测网络（或分数网络） \\
$q(\mathbf{x}_t|\mathbf{x}_{t-1})$ & 前向扩散过程（加噪） \\
$p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)$ & 逆向扩散过程（去噪） \\
$L_{\text{simple}}$ & 简化的均方误差损失 \\
\bottomrule
\end{tabular}
\end{center}

迄今为止，必须要提到的生成模型有三类：GAN、VAE 和基于流的模型（Flow-based models）。它们在生成高质量样本方面取得了巨大成功，但各有限制。GAN 因其对抗性训练性质，训练可能不稳定且生成多样性较差；VAE 依赖于代理损失；流模型则必须使用专门的架构来构建可逆变换。

扩散模型受非平衡热力学启发。它们定义了一个扩散步骤的马尔可夫链，通过缓慢地向数据添加随机噪声，然后学习逆向扩散过程，从噪声中构建所需的数据样本。与 VAE 或流模型不同，扩散模型是通过固定的过程学习的，并且隐变量具有高维度（与原始数据相同）。

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/Overview of different types of generative models.jpeg}
    \caption{不同类型生成模型的概览：GAN、VAE、Flow-based 和 Diffusion}
\end{figure}

\subsection{基本原理}

包括扩散概率模型 (Sohl-Dickstein et al., 2015)、噪声条件分数网络 (NCSN; Yang \& Ermon, 2019) 和去噪扩散概率模型 (DDPM; Ho et al. 2020) 在内的多种基于扩散的生成模型，其底层思想是相似的。

\subsubsection{前向扩散过程 (Forward Diffusion Process)}

给定从真实数据分布中采样的数据点 $\mathbf{x}_{0} \sim q(\mathbf{x})$，我们定义一个前向扩散过程，在 $T$ 个步骤中向样本添加少量高斯噪声，产生一系列噪声样本 $\mathbf{x}_{1}, \dots, \mathbf{x}_{T}$。步长由方差调度 $\{\beta_{t} \in(0,1)\}$ 控制：

\begin{equation}
q(\mathbf{x}_{t} | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_{t} ; \sqrt{1-\beta_{t}} \mathbf{x}_{t-1}, \beta_{t} \mathbf{I}) \quad q(\mathbf{x}_{1: T} | \mathbf{x}_{0}) = \prod_{t=1}^{T} q(\mathbf{x}_{t} | \mathbf{x}_{t-1})
\end{equation}

随着步长 $t$ 变大，数据样本 $\mathbf{x}_{0}$ 逐渐失去其可辨别的特征。最终当 $T \to \infty$ 时，$\mathbf{x}_{T}$ 等价于各向同性高斯分布。

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/The Markov chain of forward.jpeg}
    \caption{前向（逆向）扩散过程的马尔可夫链，通过缓慢添加（去除）噪声来生成样本}
\end{figure}

上述过程的一个优良特性是，我们可以使用重参数化技巧以闭式解在任意时间步 $t$ 对 $\mathbf{x}_{t}$ 进行采样。令 $\alpha_{t}=1-\beta_{t}$ 且 $\bar{\alpha}_{t}=\prod_{i=1}^{t} \alpha_{i}$：

\begin{equation}
\begin{aligned}
\mathbf{x}_{t} & =\sqrt{\alpha_{t}} \mathbf{x}_{t-1}+\sqrt{1-\alpha_{t}} \boldsymbol{\epsilon}_{t-1} & ;\text{where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
& =\sqrt{\alpha_{t} \alpha_{t-1}} \mathbf{x}_{t-2}+\sqrt{1-\alpha_{t} \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} & ;\text{where } \bar{\boldsymbol{\epsilon}}_{t-2} \text{ merges two Gaussians (*).} \\
& =\dots \\
& =\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}
\end{aligned}
\end{equation}

也就是：
\begin{equation}
q(\mathbf{x}_{t} | \mathbf{x}_{0})=\mathcal{N}(\mathbf{x}_{t} ; \sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0},(1-\bar{\alpha}_{t}) \mathbf{I})
\end{equation}

(*) 提示：当我们合并两个具有不同方差的高斯分布 $\mathcal{N}(\mathbf{0}, \sigma_{1}^{2} \mathbf{I})$ 和 $\mathcal{N}(\mathbf{0}, \sigma_{2}^{2} \mathbf{I})$ 时，新的分布是 $\mathcal{N}(\mathbf{0},(\sigma_{1}^{2}+\sigma_{2}^{2}) \mathbf{I})$。在这里合并的标准差是 $\sqrt{(1-\alpha_{t})+\alpha_{t}(1-\alpha_{t-1})}=\sqrt{1-\alpha_{t} \alpha_{t-1}}$。
通常，当样本变得更嘈杂时，我们可以承受更大的更新步长，所以 $\beta_{1}<\beta_{2}<\dots<\beta_{T}$，因此 $\bar{\alpha}_{1}>\dots>\bar{\alpha}_{T}$。

\subsubsection{逆向扩散过程 (Reverse Diffusion Process)}

如果我们能够逆转上述过程并从 $q(\mathbf{x}_{t-1} | \mathbf{x}_{t})$ 中采样，我们就能够从高斯噪声输入 $\mathbf{x}_{T} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ 重建真实样本。注意，如果 $\beta_{t}$ 足够小，$q(\mathbf{x}_{t-1} | \mathbf{x}_{t})$ 也将是高斯分布。遗憾的是，我们无法轻易估计 $q(\mathbf{x}_{t-1} | \mathbf{x}_{t})$，因为它需要使用整个数据集，因此我们需要学习一个模型 $p_{\theta}$ 来近似这些条件概率，以便运行逆向扩散过程。

\begin{equation}
p_{\theta}(\mathbf{x}_{0: T})=p(\mathbf{x}_{T}) \prod_{t=1}^{T} p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t}) \quad p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t})=\mathcal{N}(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_{\theta}(\mathbf{x}_{t}, t), \boldsymbol{\Sigma}_{\theta}(\mathbf{x}_{t}, t))
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/An example of training a diffusion model for modeling a 2D swiss roll data.jpeg}
    \caption{训练扩散模型以建模 2D 瑞士卷数据的示例}
\end{figure}

值得注意的是，当以 $\mathbf{x}_{0}$ 为条件时，逆向条件概率是可计算的：
\begin{equation}
q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0})=\mathcal{N}(\mathbf{x}_{t-1} ; \tilde{\boldsymbol{\mu}}(\mathbf{x}_{t}, \mathbf{x}_{0}), \tilde{\beta}_{t} \mathbf{I})
\end{equation}

使用贝叶斯规则：
\begin{equation}
\begin{aligned}
q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0}) & =q(\mathbf{x}_{t} | \mathbf{x}_{t-1}, \mathbf{x}_{0}) \frac{q(\mathbf{x}_{t-1} | \mathbf{x}_{0})}{q(\mathbf{x}_{t} | \mathbf{x}_{0})} \\
& \propto \exp \left(-\frac{1}{2}\left(\frac{(\mathbf{x}_{t}-\sqrt{\alpha_{t}} \mathbf{x}_{t-1})^{2}}{\beta_{t}}+\frac{(\mathbf{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0})^{2}}{1-\bar{\alpha}_{t-1}}-\frac{(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0})^{2}}{1-\bar{\alpha}_{t}}\right)\right) \\
& =\exp \left(-\frac{1}{2}\left(\frac{\mathbf{x}_{t}^{2}-2 \sqrt{\alpha_{t}} \mathbf{x}_{t} \mathbf{x}_{t-1}+\alpha_{t} \mathbf{x}_{t-1}^{2}}{\beta_{t}}+\frac{\mathbf{x}_{t-1}^{2}-2 \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0} \mathbf{x}_{t-1}+\bar{\alpha}_{t-1} \mathbf{x}_{0}^{2}}{1-\bar{\alpha}_{t-1}}-C(\mathbf{x}_t, \mathbf{x}_0)\right)\right) \\
& =\exp \left(-\frac{1}{2}\left(\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \mathbf{x}_{t-1}^{2}-\left(\frac{2 \sqrt{\alpha_{t}}}{\beta_{t}} \mathbf{x}_{t}+\frac{2 \sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}} \mathbf{x}_{0}\right) \mathbf{x}_{t-1}+C'(\mathbf{x}_{t}, \mathbf{x}_{0})\right)\right)
\end{aligned}
\end{equation}
其中 $C(\mathbf{x}_t, \mathbf{x}_0)$ 和 $C'(\mathbf{x}_t, \mathbf{x}_0)$ 是不包含 $\mathbf{x}_{t-1}$ 的项。

根据标准高斯密度函数，我们可以得到均值和方差的参数化形式：

\begin{equation}
\tilde{\beta}_{t} = 1 /\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \cdot \beta_{t}
\end{equation}
以及：
\begin{equation}
\begin{aligned}
\tilde{\boldsymbol{\mu}}_{t}(\mathbf{x}_{t}, \mathbf{x}_{0}) & =\left(\frac{\sqrt{\alpha_{t}}}{\beta_{t}} \mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}} \mathbf{x}_{0}\right) /\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \\
& =\left(\frac{\sqrt{\alpha_{t}}}{\beta_{t}} \mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}} \mathbf{x}_{0}\right) \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \cdot \beta_{t} \\
& =\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} \mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \mathbf{x}_{0}
\end{aligned}
\end{equation}

利用 $\mathbf{x}_{0} = \frac{1}{\sqrt{\bar{\alpha}_{t}}}(\mathbf{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}_{t})$，我们可以将均值表示为仅依赖于 $\mathbf{x}_t$ 和噪声 $\boldsymbol{\epsilon}_t$ 的形式：

\begin{equation}
\tilde{\boldsymbol{\mu}}_{t} = \frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \boldsymbol{\epsilon}_{t}\right)
\end{equation}

\subsubsection{训练目标}

这与 VAE 非常相似，因此我们可以使用变分下界（VLB）来优化负对数似然：
\begin{equation}
\begin{aligned}
-\log p_{\theta}(\mathbf{x}_{0}) & \leq -\log p_{\theta}(\mathbf{x}_{0}) + D_{\text{KL}}(q(\mathbf{x}_{1: T} | \mathbf{x}_{0}) \| p_{\theta}(\mathbf{x}_{1: T} | \mathbf{x}_{0})) \\
& = -\log p_{\theta}(\mathbf{x}_{0}) + \mathbb{E}_{\mathbf{x}_{1: T} \sim q(\mathbf{x}_{1: T} | \mathbf{x}_{0})}\left[\log \frac{q(\mathbf{x}_{1: T} | \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{0: T}) / p_{\theta}(\mathbf{x}_{0})}\right] \\
& = -\log p_{\theta}(\mathbf{x}_{0}) + \mathbb{E}_{q}\left[\log \frac{q(\mathbf{x}_{1: T} | \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{0: T})} + \log p_{\theta}(\mathbf{x}_{0})\right] \\
& = \mathbb{E}_{q}\left[\log \frac{q(\mathbf{x}_{1: T} | \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{0: T})}\right] = L_{\text{VLB}}
\end{aligned}
\end{equation}

可以将 $L_{\text{VLB}}$ 进一步重写为多个 KL 散度项的组合：

\begin{equation}
\begin{aligned}
L_{\text{VLB}} & = \mathbb{E}_{q(\mathbf{x}_{0: T})}\left[\log \frac{q(\mathbf{x}_{1: T} | \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{0: T})}\right] \\
& = \mathbb{E}_{q}\left[ \log \frac{\prod_{t=1}^{T} q(\mathbf{x}_{t} | \mathbf{x}_{t-1})}{p_{\theta}(\mathbf{x}_{T}) \prod_{t=1}^{T} p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t})} \right] \\
& = \mathbb{E}_{q}\left[ -\log p_{\theta}(\mathbf{x}_{T}) + \sum_{t=1}^{T} \log \frac{q(\mathbf{x}_{t} | \mathbf{x}_{t-1})}{p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t})} \right] \\
& = \mathbb{E}_{q}\left[ -\log p_{\theta}(\mathbf{x}_{T}) + \sum_{t=2}^{T} \log \frac{q(\mathbf{x}_{t} | \mathbf{x}_{t-1})}{p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t})} + \log \frac{q(\mathbf{x}_{1} | \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{0} | \mathbf{x}_{1})} \right] \\
& = \mathbb{E}_{q}\left[ -\log p_{\theta}(\mathbf{x}_{T}) + \sum_{t=2}^{T} \log \left( \frac{q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t})} \cdot \frac{q(\mathbf{x}_{t} | \mathbf{x}_{0})}{q(\mathbf{x}_{t-1} | \mathbf{x}_{0})} \right) + \log \frac{q(\mathbf{x}_{1} | \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{0} | \mathbf{x}_{1})} \right] \\
& = \mathbb{E}_{q}\left[ -\log p_{\theta}(\mathbf{x}_{T}) + \sum_{t=2}^{T} \log \frac{q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t})} + \sum_{t=2}^{T} \log \frac{q(\mathbf{x}_{t} | \mathbf{x}_{0})}{q(\mathbf{x}_{t-1} | \mathbf{x}_{0})} + \log \frac{q(\mathbf{x}_{1} | \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{0} | \mathbf{x}_{1})} \right] \\
& = \mathbb{E}_{q}\left[ -\log p_{\theta}(\mathbf{x}_{T}) + \sum_{t=2}^{T} \log \frac{q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t})} + \log \frac{q(\mathbf{x}_{T} | \mathbf{x}_{0})}{q(\mathbf{x}_{1} | \mathbf{x}_{0})} + \log \frac{q(\mathbf{x}_{1} | \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{0} | \mathbf{x}_{1})} \right] \\
& = \mathbb{E}_{q}\left[ \log \frac{q(\mathbf{x}_{T} | \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{T})} + \sum_{t=2}^{T} \log \frac{q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t})} - \log p_{\theta}(\mathbf{x}_{0} | \mathbf{x}_{1}) \right] \\
& = \mathbb{E}_{q}\left[ \underbrace{D_{\text{KL}}(q(\mathbf{x}_{T} | \mathbf{x}_{0}) \| p_{\theta}(\mathbf{x}_{T}))}_{L_T} + \underbrace{\sum_{t=2}^{T} D_{\text{KL}}( q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0}) \| p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t}) )}_{L_{T-1}+\dots+L_1} \underbrace{- \log p_{\theta}(\mathbf{x}_{0} | \mathbf{x}_{1})}_{L_0} \right]
\end{aligned}
\end{equation}

每一项标记如下：
\begin{equation}
L_{\text{VLB}} = L_{T} + L_{T-1} + \dots + L_{0}
\end{equation}
其中：
\begin{itemize}
    \item $L_{T}$ 是常数（因为 $q$ 无参数且 $\mathbf{x}_T$ 是高斯噪声），训练时可忽略。
    \item $L_{t}$ 是两个高斯分布之间的 KL 散度，可有闭式解。
    \item $L_{0}$ 通常使用独立的离散解码器来建模。
\end{itemize}

\subsubsection{训练损失 $L_t$ 的参数化}
我们需要学习一个神经网络来近似逆向扩散过程中的条件概率分布 $p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t})=\mathcal{N}(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_{\theta}(\mathbf{x}_{t}, t), \boldsymbol{\Sigma}_{\theta}(\mathbf{x}_{t}, t))$。我们希望训练 $\boldsymbol{\mu}_{\theta}$ 来预测 $\tilde{\boldsymbol{\mu}}_{t}=\frac{1}{\sqrt{\alpha_{t}}}(\mathbf{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \boldsymbol{\epsilon}_{t})$。
因为 $\mathbf{x}_t$ 在训练时是作为输入可用的，我们可以改为重参数化高斯噪声项，使其根据输入 $\mathbf{x}_t$ 在时间步 $t$ 预测 $\boldsymbol{\epsilon}_t$：

\begin{equation}
\boldsymbol{\mu}_{\theta}(\mathbf{x}_{t}, t)=\frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)\right)
\end{equation}

因此 $\mathbf{x}_{t-1}$ 的采样过程变为：
\begin{equation}
\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)\right) + \boldsymbol{\Sigma}_{\theta}(\mathbf{x}_{t}, t)^{1/2}\mathbf{z}
\end{equation}

损失项 $L_t$ 被参数化为最小化与 $\tilde{\boldsymbol{\mu}}_t$ 的差异：
\begin{equation}
\begin{aligned}
L_{t} & =\mathbb{E}_{\mathbf{x}_{0}, \boldsymbol{\epsilon}}\left[\frac{1}{2\left\| \boldsymbol{\Sigma}_{\theta}(\mathbf{x}_{t}, t)\right\| _{2}^{2}}\left\| \tilde{\boldsymbol{\mu}}_{t}(\mathbf{x}_{t}, \mathbf{x}_{0})-\boldsymbol{\mu}_{\theta}(\mathbf{x}_{t}, t)\right\| ^{2}\right] \\
& =\mathbb{E}_{\mathbf{x}_{0}, \boldsymbol{\epsilon}}\left[\frac{1}{2\left\| \boldsymbol{\Sigma}_{\theta}\right\| _{2}^{2}}\left\| \frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \boldsymbol{\epsilon}_{t}\right)-\frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)\right)\right\| ^{2}\right] \\
& =\mathbb{E}_{\mathbf{x}_{0}, \boldsymbol{\epsilon}}\left[\frac{(1-\alpha_{t})^{2}}{2 \alpha_{t}(1-\bar{\alpha}_{t})\left\| \boldsymbol{\Sigma}_{\theta}\right\| _{2}^{2}}\left\| \boldsymbol{\epsilon}_{t}-\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)\right\| ^{2}\right] \\
& =\mathbb{E}_{\mathbf{x}_{0}, \boldsymbol{\epsilon}}\left[\frac{(1-\alpha_{t})^{2}}{2 \alpha_{t}(1-\bar{\alpha}_{t})\left\| \boldsymbol{\Sigma}_{\theta}\right\| _{2}^{2}}\left\| \boldsymbol{\epsilon}_{t}-\boldsymbol{\epsilon}_{\theta}(\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}_{t}, t)\right\| ^{2}\right]
\end{aligned}
\end{equation}

\subsubsection{简化 (Simplification)}

Ho et al. (2020) 发现，使用忽略加权项的简化目标进行扩散模型训练效果更好：

\begin{equation}
\begin{aligned}
L_{t}^{\text{simple}} & =\mathbb{E}_{t \sim[1, T], \mathbf{x}_{0}, \boldsymbol{\epsilon}_{t}}\left[\left\| \boldsymbol{\epsilon}_{t}-\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)\right\| ^{2}\right] \\
& =\mathbb{E}_{t \sim[1, T], \mathbf{x}_{0}, \boldsymbol{\epsilon}_{t}}\left[\left\| \boldsymbol{\epsilon}_{t}-\boldsymbol{\epsilon}_{\theta}(\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}_{t}, t)\right\| ^{2}\right]
\end{aligned}
\end{equation}

最终的简单目标函数是：
\begin{equation}
L_{\text{simple}} = L_{t}^{\text{simple}} + C
\end{equation}
其中 $C$ 是一个不依赖于 $\theta$ 的常数。

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/The training and sampling algorithms in DDPM.jpeg}
    \caption{DDPM 中的训练和采样算法}
\end{figure}

\subsection{理论联系与参数化改进}

\subsubsection{与随机梯度朗之万动力学 (Langevin Dynamics) 的联系}
朗之万动力学是物理学中的一个概念，用于统计模拟分子系统。结合随机梯度下降，随机梯度朗之万动力学 (Welling \& Teh 2011) 可以仅使用梯度 $\nabla_{\mathbf{x}} \log p(\mathbf{x})$ 在马尔可夫更新链中从概率密度 $p(\mathbf{x})$ 生成样本：

\begin{equation}
\mathbf{x}_{t} = \mathbf{x}_{t-1} + \frac{\delta}{2} \nabla_{\mathbf{x}} \log p(\mathbf{x}_{t-1}) + \sqrt{\delta} \boldsymbol{\epsilon}_{t}, \quad \text{where } \boldsymbol{\epsilon}_{t} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
\end{equation}

其中 $\delta$ 是步长。当 $T \to \infty, \epsilon \to 0$ 时，$\mathbf{x}_T$ 等价于真实概率密度 $p(\mathbf{x})$。相比标准 SGD，它通过注入高斯噪声来避免陷入局部极小值。

\subsubsection{与噪声条件分数网络 (NCSN) 的联系}
Song \& Ermon (2019) 提出了基于分数的生成模型。该模型通过分数匹配（Score Matching）估计数据分布的梯度（即分数 $\nabla_{\mathbf{x}} \log q(\mathbf{x})$），并利用朗之万动力学生成样本。
为了解决高维数据在低维流形上分布导致的分数估计不准确问题，他们提出添加不同级别的高斯噪声来扰动数据，并训练一个噪声条件分数网络 $s_{\theta}(\mathbf{x}, t) \approx \nabla_{\mathbf{x}} \log q(\mathbf{x})$。

如果我们使用扩散过程的标记，分数近似为 $s_{\theta}(\mathbf{x}_{t}, t) \approx \nabla_{\mathbf{x}_{t}} \log q(\mathbf{x}_{t})$。
回忆 $q(\mathbf{x}_{t} | \mathbf{x}_{0}) \sim \mathcal{N}(\sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I})$，因此：
\begin{equation}
s_{\theta}(\mathbf{x}_{t}, t) \approx -\frac{\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)}{\sqrt{1-\bar{\alpha}_{t}}}
\end{equation}
这表明预测噪声 $\epsilon_\theta$ 本质上是在预测分数的缩放版本。

\subsubsection{$\beta_t$ 的参数化 (Parameterization of $\beta_t$)}
Ho et al. (2020) 使用线性增加的常数序列作为方差调度，从 $\beta_1=10^{-4}$ 到 $\beta_T=0.02$。
Nichol \& Dhariwal (2021) 提出了一种基于余弦的方差调度，以获得更好的对数似然（NLL）。这种调度函数在训练过程中间提供近乎线性的下降，而在 $t=0$ 和 $t=T$ 附近变化微妙。

\begin{equation}
\beta_{t}=\text{clip}\left(1-\frac{\bar{\alpha}_{t}}{\bar{\alpha}_{t-1}}, 0.999\right), \quad \bar{\alpha}_{t}=\frac{f(t)}{f(0)}, \quad f(t)=\cos \left(\frac{t / T+s}{1+s} \cdot \frac{\pi}{2}\right)^{2}
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Comparison of linear and cosine-based scheduling.jpeg}
    \caption{训练期间 $\beta_t$ 的线性调度与余弦调度的比较}
\end{figure}

\subsubsection{逆向方差 $\Sigma_\theta$ 的参数化}
Ho et al. (2020) 将 $\beta_t$ 设为常数，并设定 $\Sigma_{\theta}(\mathbf{x}_{t}, t)=\sigma_{t}^{2}\mathbf{I}$，其中 $\sigma_t$ 未学习。
Nichol \& Dhariwal (2021) 提出将 $\Sigma_{\theta}(\mathbf{x}_{t}, t)$ 学习为 $\beta_{t}$ 和 $\tilde{\beta}_{t}$ 之间的插值，模型输出一个混合向量 $v$：
\begin{equation}
\Sigma_{\theta}(\mathbf{x}_{t}, t)=\exp \left(v \log \beta_{t}+(1-v) \log \tilde{\beta}_{t}\right)
\end{equation}
这需要引入混合目标函数 $L_{\text{hybrid}} = L_{\text{simple}} + \lambda L_{\text{VLB}}$ 来指导 $\Sigma_\theta$ 的学习。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/Comparison of negative log-likelihood of improved DDPM.jpeg}
    \caption{改进版 DDPM 与其他生成模型的负对数似然 (NLL) 比较}
\end{figure}

\subsection{条件生成 (Conditioned Generation)}

虽然在 ImageNet 数据集等带有条件信息的图像上训练生成模型，但通常会根据类别标签或一段描述性文本来生成样本。

\subsubsection{分类器引导扩散 (Classifier Guided Diffusion)}
为了将类别信息显式地合并到扩散过程中，Dhariwal \& Nichol (2021) 在噪声图像 $\mathbf{x}_{t}$ 上训练了一个分类器 $f_{\phi}(y | \mathbf{x}_{t}, t)$，并使用梯度 $\nabla_{\mathbf{x}_t} \log f_{\phi}(y | \mathbf{x}_{t})$ 来引导扩散采样过程朝向条件信息 $y$（例如目标类别标签）移动。
回顾 $\nabla_{\mathbf{x}_{t}} \log q(\mathbf{x}_{t})=-\frac{1}{\sqrt{1-\bar{\alpha}_{t}}} \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)$，我们可以将联合分布 $q(\mathbf{x}_{t}, y)$ 的分数函数写为：

\begin{equation}
\begin{aligned}
\nabla_{\mathbf{x}_{t}} \log q(\mathbf{x}_{t}, y) & =\nabla_{\mathbf{x}_{t}} \log q(\mathbf{x}_{t})+\nabla_{\mathbf{x}_{t}} \log q(y | \mathbf{x}_{t}) \\
& \approx-\frac{1}{\sqrt{1-\bar{\alpha}_{t}}} \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)+\nabla_{\mathbf{x}_{t}} \log f_{\phi}(y | \mathbf{x}_{t}) \\
& =-\frac{1}{\sqrt{1-\bar{\alpha}_{t}}}\left(\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)-\sqrt{1-\bar{\alpha}_{t}} \nabla_{\mathbf{x}_{t}} \log f_{\phi}(y | \mathbf{x}_{t})\right)
\end{aligned}
\end{equation}

由此产生的扰动预测器 $\bar{\boldsymbol{\epsilon}}_{\theta}$ 形式如下：
\begin{equation}
\bar{\boldsymbol{\epsilon}}_{\theta}(\mathbf{x}_{t}, t) = \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t) - \sqrt{1-\bar{\alpha}_{t}} \nabla_{\mathbf{x}_{t}} \log f_{\phi}(y | \mathbf{x}_{t})
\end{equation}
为了控制分类器引导的强度，我们可以向 delta 部分添加权重 $w$：
\begin{equation}
\bar{\boldsymbol{\epsilon}}_{\theta}(\mathbf{x}_{t}, t) = \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t) - \sqrt{1-\bar{\alpha}_{t}} w \nabla_{\mathbf{x}_{t}} \log f_{\phi}(y | \mathbf{x}_{t})
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/The algorithms use guidance from a classifier to run conditionedgeneration with DDPM and DDIM.jpeg}
    \caption{使用分类器引导来进行条件生成的算法 (DDPM 和 DDIM)}
\end{figure}

\subsubsection{无分类器引导 (Classifier-Free Guidance)}
即使没有独立的分类器 $f_{\phi}$，也可以通过结合条件和无条件扩散模型的分数来运行条件扩散步骤 (Ho \& Salimans, 2021)。具体来说，我们在成对数据 $(\mathbf{x}, y)$ 上训练条件扩散模型 $p_{\theta}(\mathbf{x} | y)$，其中条件信息 $y$ 会随机被丢弃，以便模型也学会无条件地生成图像，即 $\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t) = \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t, y=\emptyset)$。

隐式分类器的梯度可以用条件和无条件分数估计器来表示：
\begin{equation}
\begin{aligned}
\nabla_{\mathbf{x}_{t}} \log p(y | \mathbf{x}_{t}) & =\nabla_{\mathbf{x}_{t}} \log p(\mathbf{x}_{t} | y)-\nabla_{\mathbf{x}_{t}} \log p(\mathbf{x}_{t}) \\
& =-\frac{1}{\sqrt{1-\bar{\alpha}_{t}}}\left(\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t, y)-\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)\right)
\end{aligned}
\end{equation}
将其代入分类器引导的分数公式中，分数便不再依赖单独的分类器：
\begin{equation}
\begin{aligned}
\bar{\boldsymbol{\epsilon}}_{\theta}(\mathbf{x}_{t}, t, y) & =\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t, y)-\sqrt{1-\bar{\alpha}_{t}} w \nabla_{\mathbf{x}_{t}} \log p(y | \mathbf{x}_{t}) \\
& =\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t, y)+w\left(\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t, y)-\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)\right) \\
& =(w+1) \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t, y)-w \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)
\end{aligned}
\end{equation}

实验表明，无分类器引导可以在 FID（图像保真度）和 IS（多样性）之间取得良好的平衡。GLIDE (Nichol, Dhariwal \& Ramesh, et al. 2022) 发现无分类器引导优于 CLIP 引导。

\subsection{加速扩散模型 (Speed up Diffusion Models)}
从 DDPM 生成样本非常慢，因为 $T$ 可能高达 1000 步。一重简单的加速方法是运行跨步采样（Strided Sampling），例如每隔 $\lceil T/S \rceil$ 步进行一次更新。

另一种方法重写 $q_{\sigma}(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0})$，使其由所需的标准差 $\sigma_t$ 参数化：
\begin{equation}
\begin{aligned}
\mathbf{x}_{t-1} & =\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t-1}} \boldsymbol{\epsilon}_{t-1} \\
& =\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_{t}^{2}} \boldsymbol{\epsilon}_{t}+\sigma_{t} \boldsymbol{\epsilon} \\
& =\sqrt{\bar{\alpha}_{t-1}}\left(\frac{\mathbf{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}_{\theta}^{(t)}(\mathbf{x}_{t})}{\sqrt{\bar{\alpha}_{t}}}\right)+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_{t}^{2}} \boldsymbol{\epsilon}_{\theta}^{(t)}(\mathbf{x}_{t})+\sigma_{t} \boldsymbol{\epsilon}
\end{aligned}
\end{equation}
这样我们得到：
\begin{equation}
q_{\sigma}(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0})=\mathcal{N}\left(\mathbf{x}_{t-1} ; \sqrt{\bar{\alpha}_{t-1}}\left(\frac{\mathbf{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}_{\theta}^{(t)}(\mathbf{x}_{t})}{\sqrt{\bar{\alpha}_{t}}}\right)+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_{t}^{2}} \boldsymbol{\epsilon}_{\theta}^{(t)}(\mathbf{x}_{t}), \sigma_{t}^{2} \mathbf{I}\right)
\end{equation}

回忆 $\tilde{\beta}_{t} = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \cdot \beta_{t}$，令 $\sigma_{t}^{2}=\eta \cdot \tilde{\beta}_{t}$，我们可以通过超参数 $\eta \in \mathbb{R}^+$ 控制采样随机性。

\subsubsection{DDIM (Denoising Diffusion Implicit Models)}
当 $\eta=0$ 时，采样过程变得确定性，此时模型称为去噪扩散隐模型 (DDIM)。
DDIM 更新步骤（针对加速轨迹中的 $s < t$）：
\begin{equation}
q_{\sigma, s<t}(\mathbf{x}_{s} | \mathbf{x}_{t}, \mathbf{x}_{0})=\mathcal{N}\left(\mathbf{x}_{s} ; \sqrt{\bar{\alpha}_{s}}\left(\frac{\mathbf{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}_{\theta}^{(t)}(\mathbf{x}_{t})}{\sqrt{\bar{\alpha}_{t}}}\right)+\sqrt{1-\bar{\alpha}_{s}-\sigma_{t}^{2}} \boldsymbol{\epsilon}_{\theta}^{(t)}(\mathbf{x}_{t}), \sigma_{t}^{2} \mathbf{I}\right)
\end{equation}

DDIM 具有一致性属性，即以相同的隐变量为条件的多个样本应具有相似的高级特征。这也是因为 DDIM 可以在隐变量中进行语义上有意义的插值。

\subsubsection{渐进式蒸馏 (Progressive Distillation)}
Salimans \& Ho (2022) 提出了一种将训练好的确定性采样器蒸馏成步数减半的新模型的方法。在每次蒸馏迭代中，我们可以将采样步骤减半。
学生模型初始化自教师模型，并朝着匹配 2 个教师 DDIM 步骤的目标进行去噪（即算法中的 $z_{t''}$），而不是使用原始样本 $\mathbf{x}_0$ 作为去噪目标。

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/Progressive distillation can reduce the diffusion sampling steps by half ineach iteration.jpeg}
    \caption{渐进式蒸馏可以在每次迭代中将扩散采样步骤减半}
\end{figure}

\subsubsection{一致性模型 (Consistency Models)}
Song et al. (2023) 提出的一致性模型旨在学习将扩散采样轨迹上的任何中间噪声数据点 $\mathbf{x}_{t}$ 直接映射回其原点 $\mathbf{x}_{0}$。如名称所示，它具有自一致性。

一致性模型定义为 $f_\theta: (\mathbf{x}_t, t) \mapsto \mathbf{x}_\epsilon$，使得对同一轨迹上的所有点输出相同。
它可以被参数化为：
\begin{equation}
f_\theta(\mathbf{x}, t) = c_{\text{skip}}(t)\mathbf{x} + c_{\text{out}}(t)F_\theta(\mathbf{x}, t)
\end{equation}

该论文介绍了两种训练一致性模型的方法：
\begin{enumerate}
    \item \textbf{一致性蒸馏 (Consistency Distillation, CD)}: 将预训练的扩散模型蒸馏为一致性模型。损失函数为：
    \begin{equation}
    \mathcal{L}_{CD}^N(\theta, \theta^-; \phi) = \mathbb{E}[\lambda(t_n)d(f_\theta(\mathbf{x}_{t_{n+1}}, t_{n+1}), f_{\theta^-}(\mathbf{x}_{t_n}^\phi, t_n))]
    \end{equation}
    其中 $\theta^-$ 是 $\theta$ 的 EMA 版本。

    \item \textbf{一致性训练 (Consistency Training, CT)}: 独立训练一致性模型。损失函数为：
    \begin{equation}
    \mathcal{L}_{CT}^N(\theta, \theta^-; \phi) = \mathbb{E}[\lambda(t_n)d(f_\theta(\mathbf{x} + t_{n+1}\mathbf{z}, t_{n+1}), f_{\theta^-}(\mathbf{x} + t_n \mathbf{z}, t_n))]
    \end{equation}
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/Consistency models learn to map any data point on the trajectory back toits origin.jpeg}
    \caption{一致性模型学习将轨迹上的任何数据点映射回其原点}
\end{figure}

\subsection{隐变量空间 (Latent Variable Space)}

潜在扩散模型 (Latent Diffusion Model, LDM; Rombach et al. 2022) 在潜在空间而不是像素空间中运行扩散过程，从而降低了训练成本并加快了推理速度。其动机是观察到图像的大部分比特贡献了感知细节，而语义和概念构成在激进压缩后仍然保留。

LDM 将感知压缩（使用自动编码器去除像素级冗余）和语义压缩（在学习到的隐变量上使用扩散过程生成语义概念）松散地分解开来。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/The plot for tradeoff between compression rate and distortion, illustratingtwo-stage compressions - perceptual and semantic compression.jpeg}
    \caption{压缩率与失真之间的权衡，说明了两阶段压缩：感知压缩和语义压缩}
\end{figure}

感知压缩依赖于一个自编码器模型。扩散和去噪过程发生在隐向量 $z$ 上。去噪模型是一个时间条件的 U-Net，增强了交叉注意力机制（Cross-Attention）以处理灵活的条件信息（如类别标签、语义图等）。
条件输入 $y$ 被投影到中间表示 $\tau_\theta(y) \in \mathbb{R}^{M \times d_\tau}$，然后映射到交叉注意力组件：
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right) \cdot V
\end{equation}
其中：
\begin{equation}
Q = W_Q^{(i)} \cdot \varphi_i(z_i),\ K = W_K^{(i)} \cdot \tau_\theta(y),\ V = W_V^{(i)} \cdot \tau_\theta(y)
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/The architecture of the latent diffusion model.jpeg}
    \caption{潜在扩散模型 (LDM) 的架构}
\end{figure}

\subsection{扩大生成分辨率与质量}

为了生成高分辨率的高质量图像，Ho et al. (2021) 提出使用多个扩散模型的级联管道。管道模型之间的噪声条件增强（Noise Conditioning Augmentation）对于最终图像质量至关重要。

\subsubsection{级联扩散模型 (Cascaded Diffusion Models)}
级联管道包括一个生成低分辨率图像的基础模型，以及随后的一个或多个超分辨率（Super-Resolution）扩散模型。
研究发现，最有效的调节噪声是在低分辨率下施加高斯噪声，在高分辨率下施加高斯模糊。

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/A cascaded pipeline of multiple diffusion models at increasingresolutions.jpeg}
    \caption{多个扩散模型在增加分辨率时的级联管道}
\end{figure}

\subsubsection{unCLIP (DALL·E 2)}
unCLIP (Ramesh et al. 2022) 是一种利用 CLIP 文本编码器的两阶段扩散模型。它并行学习两个模型：
\begin{enumerate}
    \item 先验模型 (Prior) $P(c^i|y)$：给定文本 $y$，输出 CLIP 图像嵌入 $c^i$。
    \item 解码器 (Decoder) $P(x|c^i, [y])$：给定 CLIP 图像嵌入 $c^i$（以及可选的原始文本 $y$），生成图像 $x$。
\end{enumerate}

这两个模型通过贝叶斯规则实现条件生成：
\begin{equation}
P(x|y) = \underbrace{P(x, c^i|y)}_{c^i \text{ is deterministic given } x} = P(x|c^i, y)P(c^i|y)
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/The architecture of unCLIP.jpeg}
    \caption{unCLIP 的架构}
\end{figure}

\subsubsection{Imagen}
Imagen (Saharia et al. 2022) 使用预训练的大型语言模型（即冻结的 T5-XXL 文本编码器）对文本进行编码。研究发现，扩大文本编码器的规模比扩大 U-Net 的规模对图像质量更重要。

\subsection{模型架构 (Model Architecture)}

扩散模型有两个常见的骨干架构选择：U-Net 和 Transformer。

\subsubsection{U-Net}
U-Net (Ronneberger, et al. 2015) 由下采样堆栈和上采样堆栈组成，中间通过快捷连接（Shortcut Connections）相连，提供高分辨率特征。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/The U-net architecture.jpeg}
    \caption{U-Net 架构}
\end{figure}

\subsubsection{ControlNet}
为了通过额外的图像（如 Canny 边缘、Hough 线、人体骨架图等）对图像生成进行条件控制，ControlNet (Zhang et al. 2023) 引入了架构更改。它主要包括两步：锁定原始模型参数，并创建一个可训练的副本；通过“零卷积”层连接这两个块。零卷积层的权重和偏置初始化为零，从而在训练初始阶段保护骨干网络不受随机噪声梯度的影响。
对于神经网络块 $F_\theta(\cdot)$，ControlNet 首先冻结 $\theta$，然后克隆它得到带有参数 $\theta_c$ 的副本。零卷积层 $Z_{\theta_{z1}}$ 和 $Z_{\theta_{z2}}$ 连接这两个块：
\begin{equation}
y_c = F_\theta(\mathbf{x}) + Z_{\theta_{z2}}(F_{\theta_c}(\mathbf{x} + Z_{\theta_{z1}}(\mathbf{c})))
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/The ControlNet architecture.jpeg}
    \caption{ControlNet 架构}
\end{figure}

\subsubsection{Diffusion Transformer (DiT)}
Diffusion Transformer (DiT; Peebles \& Xie, 2023) 在潜在图块（Latent Patches）上运行。其步骤如下：
1. 将输入 $z$ 的潜在表示作为输入。
2. 将噪声潜在表示“Patchify”为序列。
3. 输入 Transformer 块。
DiT 使用 adaLN-Zero（自适应层归一化）机制来注入条件信息（如时间步 $t$ 和类别 $c$）。DiT 的一大优势是其性能随着计算量的增加和模型规模的扩大而有效扩展。

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/The Diffusion Transformer.jpeg}
    \caption{Diffusion Transformer (DiT) 架构}
\end{figure}

\nocite{*}
\bibliography{references}
\end{document}
